{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking Article Processing Charges (APCs) for a given institution\n",
    "\n",
    "In this notebook, we will query the OpenAlex API to answer the following questions:  \n",
    "\n",
    "1. **How much are researchers at my institution paying in APCs?**\n",
    "2. **Which journals/publishers are collecting the most APCs from researchers at my institution?**\n",
    "3. **How much money are my organizationâ€™s researchers saving in discounted APC charges from our transformative/read-publish agreements?**\n",
    "\n",
    "Most organizations do not have an effective way of tracking the APCs that their researchers pay to publish in open access journals.  By estimating how much money is going to APCs each year, and which publishers are collecting the most APCs, libraries can make more informed decisions around the details of the read-publish agreements they have with various publishers.  \n",
    "\n",
    "### APC-able Works\n",
    "\n",
    "Before starting this analysis, it is important to define which types of works are subject to APCs and which are not.   \n",
    "\n",
    "While a work may include contributions from a number of different institutions, the APC is typically the responsbility of the work's *corresponding author*.  \n",
    "\n",
    "In addition, open access works published in Gold and Hybrid OA journals are subject to APCs, while those published in Green, Diamond, and Bronze OA journals are not.  \n",
    "\n",
    "Finally, APCs are not typically charged for editorial content submitted to an open access journal.  \n",
    "\n",
    "Thus, for the purposes of this notebook, *APC-able works* must have the following characteristics:\n",
    "- Original articles or reviews\n",
    "- Published in a Gold or Hybrid OA journal\n",
    "- Corresponding author is a researcher at our institution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surveying APCs by journal/publisher\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. We need to get all the works published and corresponded by researchers at the institution\n",
    "2. We get the journal/publisher and APC for each publication\n",
    "3. We sum the APCs (by journal/publisher)\n",
    "\n",
    "### Input\n",
    "\n",
    "For inputs, we first need to identify the Research Organization Registry (ROR) ID for our institution. In this example we will use the ROR ID for McMaster University ([https://ror.org/02fa3aq29](https://ror.org/02fa3aq29)). You can search and substitute your own institution's ROR here: [https://ror.org/search](https://ror.org/search).  \n",
    "\n",
    "Next, we identify the publication year we are interested in analyzing. If the details of your institution's specific tranformative/read-publish agreements change from year to year, you will want to limit your analysis to a single year.  \n",
    "\n",
    "Finally, becauase editorial content is not typically subject to APCs, we will limit our search to works with the publication types \"article\" or \"review\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_OPTIONAL = True  # flag to determine whether to run optional code\n",
    "SAVE_CSV = True  # flag to determine whether to save the output as a CSV file\n",
    "FIRST_NUM_ROWS = 10  # number of top items to display\n",
    "\n",
    "# input\n",
    "ror_ids = [\n",
    "    \"https://ror.org/02fa3aq29\",  # McMaster University\n",
    "    \"https://ror.org/03cegwq60\",  # McMaster Children's Hospital\n",
    "    \"https://ror.org/02zrdtc90\",  # McMaster Divintiy College\n",
    "    \"https://ror.org/05jyrng31\",  # McMaster University Medical Center\n",
    "]\n",
    "publication_year = 2024\n",
    "publication_types = [\"article\", \"review\"]\n",
    "publication_oa_statuses = [\"gold\", \"hybrid\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get OpenAlex ID of the given institution\n",
    "\n",
    "We only want publications with corresponding authors, who are affiliated with McMaster University. However, OpenAlex currently does not support filtering corresponding institutions by ROR ID, we will need to find out the OpenAlex ID for McMaster using the [`institutions`](https://docs.openalex.org/api-entities/institutions) entity type.  \n",
    "\n",
    "Our search criteria are as follows:  \n",
    "- `ror`: ROR ID of the institution, `ror:https://ror.org/02fa3aq2`\n",
    "\n",
    "Now we need to build an URL for the query from the following parameters:  \n",
    "- Starting point is the base URL of the OpenAlex API: `https://api.openalex.org/`\n",
    "- We append the entity type to it: `https://api.openalex.org/institutions`\n",
    "- All criteria need to go into the query parameter filter that is added after a question mark: `https://api.openalex.org/institutions?filter=`\n",
    "- To construct the filter value we take the criteria we specified and concatenate them using commas as separators: `https://api.openalex.org/institutions?filter=ror:https://ror.org/02fa3aq29`\n",
    "\n",
    "```py\n",
    "# construct the url using the provided ror id\n",
    "url = f\"https://api.openalex.org/institutions?filter=ror:{ror_id}\"\n",
    "\n",
    "# send a get request to the constructed url\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the response json data\n",
    "json_data = response.json()\n",
    "\n",
    "# extract the institution id from the first result\n",
    "institution_id = json_data[\"results\"][0][\"id\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openalex_helpers.institutions import get_institution_by_ror\n",
    "\n",
    "institution_ids = [get_institution_by_ror(ror_id)[\"id\"] for ror_id in ror_ids]\n",
    "institution_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all APC-able works published by researchers at the institution\n",
    "\n",
    "Our search criteria are as follows:  \n",
    "- `corresponding_institution_ids`: institution affiliated with the corresponding authors of a work (OpenAlex ID), `corresponding_institution_ids:https://openalex.org/I98251732|https://openalex.org/I2801880889|https://openalex.org/I163387037|https://openalex.org/I2801259263`\n",
    "- `publication_year`: the year the work was published, `publication_year:2024`\n",
    "- [`types`](https://docs.openalex.org/api-entities/works/work-object#type): the type of the work, `type:article|review`\n",
    "- [`oa_status`](https://docs.openalex.org/api-entities/works/work-object#open_access): the OA status of the work, `oa_status:gold|hybrid`\n",
    "\n",
    "Now we need to build an URL for the query from the following parameters:  \n",
    "- Starting point is the base URL of the OpenAlex API: `https://api.openalex.org/`\n",
    "- We append the entity type to it: `https://api.openalex.org/works`\n",
    "- All criteria need to go into the query parameter filter that is added after a question mark: `https://api.openalex.org/works?filter=`\n",
    "- To construct the filter value we take the criteria we specified and concatenate them using commas as separators: `https://api.openalex.org/works?filter=corresponding_institution_ids:https://openalex.org/I98251732,publication_year:2024,type:article|review,oa_status:gold|hybrid&page=1&per-page=50`\n",
    "\n",
    "\n",
    "```py\n",
    "def get_works_by_corresponding_institutions(institution_ids, publication_year, publication_types, page=1, items_per_page=50):\n",
    "    # construct the api url with the given institution ids, publication year, publication types, page number, and items per page\n",
    "    url = f\"https://api.openalex.org/works?filter=corresponding_institution_ids:{'|'.join(institution_ids)},publication_year:{publication_year},type:{'|'.join(publication_types)},oa_status:{'|'.join(publication_oa_statuses)}&page={page}&per-page={items_per_page}\"\n",
    "\n",
    "    # send a GET request to the api and parse the json response\n",
    "    response = requests.get(url)\n",
    "    json_data = response.json()\n",
    "\n",
    "    # convert the json response to a dataframe\n",
    "    df_json = pd.DataFrame.from_dict(json_data[\"results\"])\n",
    "\n",
    "    next_page = True\n",
    "    if df_json.empty: # check if the dataframe is empty (i.e., no more pages available)\n",
    "        next_page = False\n",
    "\n",
    "    # if there are more pages, recursively fetch the next page\n",
    "    if next_page:\n",
    "        df_json_next_page = get_works_by_corresponding_institutions(institution_ids, publication_year, publication_types, page=page+1, items_per_page=items_per_page)\n",
    "        df_json = pd.concat([df_json, df_json_next_page])\n",
    "\n",
    "    return df_json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openalex_helpers.works import get_works_by_corresponding_institutions\n",
    "\n",
    "try:\n",
    "    from helpers.converters import to_dict_convertor\n",
    "    # read the institution_works_{publication_year}.csv file if it exists, otherwise query data from OpenAlex\n",
    "    df_works = pd.read_csv(f\"data/apc/institution_works_{publication_year}.csv\", converters={ \"apc_list\": to_dict_convertor, \"primary_location\": to_dict_convertor })  # use converters to parse 'apc_list' and 'primary_location' columns as dictionaries\n",
    "except FileNotFoundError:\n",
    "    df_works = get_works_by_corresponding_institutions(\n",
    "        institution_ids, publication_year, publication_types, publication_oa_statuses\n",
    "    )\n",
    "\n",
    "    if SAVE_CSV:\n",
    "        df_works.to_csv(f\"data/apc/institution_works_{publication_year}.csv\", index=False)\n",
    "\n",
    "df_works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Journals/Publishers and APCs in USD\n",
    "\n",
    "In a `work` entity object, there is information about the journal (`primary_location`) and the journal's APC list price ([`apc_list`](https://docs.openalex.org/api-entities/works/work-object#apc_list)).  \n",
    "\n",
    "is derived from the [Directory of Open Access Journals (DOAJ)](https://doaj.org/), which compiles APC data currently available on publishers' websites.  \n",
    "\n",
    "It should be noted that not all publishers list APC prices on their websites, meaning that not all works will have an `apc_list` price in OpenAlex.  In these cases, we will infer the APC price based on the mean APC prices of those works for which `apc_list` data is available.  \n",
    "\n",
    "In addition, even when APC price is included on a publishers' website, there is no guarantee that this is the final APC price our authors paid for publication.  \n",
    "\n",
    "For these reasons, results of this notebook must be understood as best available estimates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# extract 'value_usd' from 'apc_list' if it is a dictionary (i.e. 'apc_list' exists in the work record); otherwise, set to null\n",
    "df_works[\"apc_list_usd\"] = df_works[\"apc_list\"].apply(lambda apc_list: apc_list[\"value_usd\"] if isinstance(apc_list, dict) else np.nan)\n",
    "\n",
    "# extract 'id' and 'name' from 'source' within 'primary_location' if 'source' exists; otherwise, set to null\n",
    "df_works[\"source_id\"] = df_works[\"primary_location\"].apply(lambda location: location[\"source\"][\"id\"] if location[\"source\"] else np.nan)\n",
    "df_works[\"source_name\"] = df_works[\"primary_location\"].apply(lambda location: location[\"source\"][\"display_name\"] if location[\"source\"] else np.nan)\n",
    "\n",
    "# extract 'host_organization' from 'source' within 'primary_location' if 'source' exists; otherwise, set to null\n",
    "df_works[\"source_host_organization\"] = df_works[\"primary_location\"].apply(lambda location: location[\"source\"][\"host_organization\"] if location[\"source\"] else np.nan)\n",
    "\n",
    "# extract 'issn' and 'issn_l' from 'source' within 'primary_location' if 'source' exists; otherwise, set to null\n",
    "df_works[\"source_issn\"] = df_works[\"primary_location\"].apply(lambda location: location[\"source\"][\"issn\"] if location[\"source\"] else np.nan)\n",
    "df_works[\"source_issn_l\"] = df_works[\"primary_location\"].apply(lambda location: location[\"source\"][\"issn_l\"] if location[\"source\"] else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average apc where 'apc_list_usd' is not null\n",
    "apc_mean = df_works[df_works[\"apc_list_usd\"].notnull()][\"apc_list_usd\"].mean()\n",
    "\n",
    "# fill null values in 'apc_list_usd' with the calculated average\n",
    "df_works[\"apc_list_usd\"] = df_works[\"apc_list_usd\"].fillna(apc_mean)\n",
    "\n",
    "# fill null values in 'source_id', 'source_name', 'source_issn' and 'source_issn_l'\n",
    "df_works[\"source_id\"] = df_works[\"source_id\"].fillna(\"unknown source\")\n",
    "df_works[\"source_name\"] = df_works[\"source_name\"].fillna(\"unknown source\")\n",
    "df_works[\"source_host_organization\"] = df_works[\"source_host_organization\"].fillna(\"unknown source\")\n",
    "df_works[\"source_issn\"] = df_works[\"source_issn\"].fillna(\"unknown source\")\n",
    "df_works[\"source_issn_l\"] = df_works[\"source_issn_l\"].fillna(\"unknown source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Publisher Display Name (Optional)\n",
    "\n",
    "OpenAlex identifies publishers with a unique identfier called an OpenAlex ID. The following code translates this OpenAlex ID to the publisher's display name for easier analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "CHUNK_SIZE = 5\n",
    "\n",
    "def get_source_host_organization_display_name(publisher_ids):\n",
    "    def get_source_host_organization_publisher_display_name(publisher_ids):\n",
    "        def get_source_host_organization_publisher_display_name_by_chunk(publisher_ids_chunk):\n",
    "            # construct the api url using the chunk of publisher ids\n",
    "            url = f\"https://api.openalex.org/publishers?filter=ids.openalex:{'|'.join(publisher_ids_chunk)}\"\n",
    "\n",
    "            # send a GET request to the api and parse the json response\n",
    "            response = requests.get(url)\n",
    "            json_data = response.json()\n",
    "\n",
    "            # convert the json response to a dataframe and return the relevant columns\n",
    "            df_json = pd.DataFrame.from_dict(json_data[\"results\"])\n",
    "            return df_json[[\"id\", \"display_name\"]]\n",
    "        \n",
    "        # check if the length of 'publisher_ids' is less than 1\n",
    "        if len(publisher_ids) < 1:\n",
    "            # if true, return an empty dataframe\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # split the publisher ids into chunks and apply the function to each chunk\n",
    "        chunks = np.array_split(publisher_ids, np.ceil(len(publisher_ids) / CHUNK_SIZE))\n",
    "        df_chunks = pd.DataFrame({\"chunk\": chunks})\n",
    "        return pd.concat(df_chunks[\"chunk\"].apply(get_source_host_organization_publisher_display_name_by_chunk).tolist())\n",
    "\n",
    "    def get_source_host_organization_institution_display_name(institution_ids):\n",
    "        def get_source_host_organization_institution_display_name_by_chunk(institution_ids_chunk):\n",
    "            # construct the api url using the chunk of institution ids\n",
    "            url = f\"https://api.openalex.org/institutions?filter=id:{'|'.join(institution_ids_chunk)}\"\n",
    "\n",
    "            # send a GET request to the api and parse the json response\n",
    "            response = requests.get(url)\n",
    "            json_data = response.json()\n",
    "\n",
    "            # convert the json response to a dataframe and return the relevant columns\n",
    "            df_json = pd.DataFrame.from_dict(json_data[\"results\"])\n",
    "            return df_json[[\"id\", \"display_name\"]]\n",
    "\n",
    "        # check if the length of 'institution_ids' is less than 1\n",
    "        if len(institution_ids) < 1:\n",
    "            # if true, return an empty dataframe\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # split the institution ids into chunks and apply the function to each chunk\n",
    "        chunks = np.array_split(institution_ids, np.ceil(len(institution_ids) / CHUNK_SIZE))\n",
    "        df_chunks = pd.DataFrame({\"chunk\": chunks})\n",
    "        return pd.concat(df_chunks[\"chunk\"].apply(get_source_host_organization_institution_display_name_by_chunk).tolist())\n",
    "\n",
    "     # filter the publisher ids to get only publisher urls\n",
    "    publishers = list(filter(lambda s: re.search(r\"https:\\/\\/openalex\\.org\\/P\", s), publisher_ids))\n",
    "    # filter the institution ids to get only institution urls\n",
    "    institutions = list(filter(lambda s: re.search(r\"https:\\/\\/openalex\\.org\\/I\", s), publisher_ids))\n",
    "\n",
    "    # create a dataframe with a default entry for unknown source\n",
    "    df_lookup = pd.DataFrame.from_dict({\"id\": [\"unknown source\"], \"display_name\": [\"unknown source\"]})\n",
    "    # concatenate the dataframes with publisher and institution display names\n",
    "    df_lookup = pd.concat([df_lookup, get_source_host_organization_publisher_display_name(publishers), get_source_host_organization_institution_display_name(institutions)], ignore_index=True)\n",
    "    return df_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_OPTIONAL:\n",
    "    # read the source_host_organization_lookup.csv file if it exists, otherwise query data from OpenAlex\n",
    "    try:\n",
    "        df_lookup = pd.read_csv(f\"data/apc/source_host_organization_lookup.csv\")\n",
    "    except FileNotFoundError:\n",
    "        # get the display names for unique source_host_organization ids in df_works\n",
    "        df_lookup = get_source_host_organization_display_name(df_works[\"source_host_organization\"].unique())\n",
    "        if SAVE_CSV:\n",
    "            df_lookup.to_csv(f\"data/apc/source_host_organization_lookup.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_OPTIONAL:\n",
    "    # update the 'source_host_organization' with the corresponding display names\n",
    "    df_works[\"source_host_organization\"] = df_works[\"source_host_organization\"].apply(lambda publisher: df_lookup[df_lookup[\"id\"] == publisher][\"display_name\"].squeeze())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Journals/Publishers and APCs in USD (Continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the 'apc_list_usd' column to 'apc_usd'\n",
    "df_apc = df_works.rename(columns={\"apc_list_usd\": \"apc_usd\"})\n",
    "if SAVE_CSV:\n",
    "    df_apc.to_csv(f\"data/apc/apc_usd.csv\", index=False)\n",
    "\n",
    "df_apc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate APCs Data (Optional)\n",
    "\n",
    "Here, we build a dataframe containing the number of APC-able works and the estiamted total APC cost for each journal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_OPTIONAL:\n",
    "    # group the dataframe by 'source_id' and 'source_issn_l'\n",
    "    # and aggregate 'source_issn' by taking the maximum value (in this case the common issn list of strings)\n",
    "    # and 'source_host_organization' by taking the maximum value (in this case the common string name of the source's host organization)\n",
    "    # and 'source_name' by taking the maximum value (in this case the common string name of the source)\n",
    "    # and 'id' by counting\n",
    "    # and 'apc_list_usd' by summing\n",
    "    df_apc = df_works.groupby([\"source_id\", \"source_issn_l\"]).agg({\"source_issn\": \"max\", \"source_name\": \"max\", \"source_host_organization\": \"max\", \"id\": \"count\", \"apc_list_usd\": \"sum\"})\n",
    "    # rename the 'id' column to 'num_publications' and 'apc_list_usd' column to 'apc_usd'\n",
    "    df_apc.rename(columns={\"id\": \"num_publications\", \"apc_list_usd\": \"apc_usd\"}, inplace=True)\n",
    "    if SAVE_CSV:\n",
    "        df_apc.to_csv(f\"data/apc/apc_usd_by_source.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the total (non-discounted) APC spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_apc = df_apc[\"apc_usd\"].sum()\n",
    "print(f\"Estimated total (non-discounted) APC cost in {publication_year}: USD {round(total_apc, 2)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Discounted APCs\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. We load the given list of read-publish agreement discounts\n",
    "2. We check if publishers are included in the list by ISSN\n",
    "3. We calculate the APCs paid with the list of read-publish agreement discounts and the APC listed prior\n",
    "\n",
    "### Input\n",
    "\n",
    "Assume we have list of read-publish agreement discounts in CSV format, `discount-list.csv`. In the file, it includes the following necessary attributes,  \n",
    "- `issn`: ISSN of the publisher\n",
    "- `discount`: value of the discount, either a number or a percentage\n",
    "- `is_flatrate`: flag indicating whether the discount is a flat rate discount or a percentage discount\n",
    "\n",
    "You can download a template [`data/apc/discount-list.csv`](data/apc/discount-list.csv.example) here and update it with the details of your institutions own APC discounts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "df_discount = pd.read_csv(\"data/apc/discount-list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "def get_discount(issn: typing.List[str] | str, apc: float, num_publications: typing.Optional[int]) -> float:\n",
    "    # check if issn is a string, if so, convert it to a list\n",
    "    if isinstance(issn, str):\n",
    "        issn = [issn]\n",
    "\n",
    "    # filter the discount dataframe to get rows where 'issn' is in the provided issn list\n",
    "    discount_rows = df_discount[df_discount[\"issn\"].isin(issn)]\n",
    "    \n",
    "    # if no discount rows are found, return the original apc\n",
    "    if discount_rows.empty:\n",
    "        return apc\n",
    "\n",
    "    # get the first row from the filtered discount rows\n",
    "    discount_row = discount_rows.iloc[0]\n",
    "    \n",
    "    # if the discount is a flat rate, subtract the discount from the apc\n",
    "    if discount_row[\"is_flatrate\"]:\n",
    "        return apc - discount_row[\"discount\"] * (num_publications if num_publications is not None else 1)  # discount is applied per publication\n",
    "    else:\n",
    "        # if the discount is a percentage, apply the discount to the apc\n",
    "        return apc * (1 - discount_row[\"discount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Discounts to APC Data\n",
    "\n",
    "Here, we apply the APC discounts to the APC data. This produces a dataframe and `.csv` file that includes the number of APC-able publications and the discounted APC cost for each journal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the get_discount function to each row of the dataframe to calculate the discounted apc and store it in a new column 'discounted_apc_usd'\n",
    "df_apc[\"discounted_apc_usd\"] = df_apc.apply(lambda x: get_discount(issn=x[\"source_issn\"], apc=x[\"apc_usd\"], num_publications=(x[\"num_publications\"] if \"num_publications\" in df_apc.columns else None)), axis=1)\n",
    "if SAVE_CSV:\n",
    "    df_apc.to_csv(f\"data/apc/apc_usd_with_discounts.csv\", index=False)\n",
    "\n",
    "df_apc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Top Publishers by APC Saved (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_OPTIONAL:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # group the dataframe by 'source_host_organization'\n",
    "    # and aggregate 'num_publications' by summing\n",
    "    # and 'apc_usd' by summing\n",
    "    # and 'discounted_apc_usd' by summing\n",
    "    _df_apc = df_apc.groupby([\"source_host_organization\"]).agg({\"num_publications\": \"sum\", \"apc_usd\": \"sum\", \"discounted_apc_usd\": \"sum\"})\n",
    "    # calculate the total apc savings by subtracting discounted apc from original apc\n",
    "    _df_apc[\"apc_saved\"] = _df_apc[\"apc_usd\"] - _df_apc[\"discounted_apc_usd\"]\n",
    "\n",
    "    if SAVE_CSV:\n",
    "        _df_apc.to_csv(f\"data/apc/apc_usd_with_discounts_by_source_host_organization.csv\", index=False)\n",
    "\n",
    "    # sort the dataframe by 'discounted_apc_usd' in descending order and select the top rows\n",
    "    _df_apc = _df_apc.sort_values(by=\"apc_saved\", ascending=False).head(FIRST_NUM_ROWS)\n",
    "\n",
    "    # create the figure and the first y-axis\n",
    "    fig, ax1 = plt.subplots()\n",
    "    # create a second y-axis that shares the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # define the width of each bar and the positions for the bars\n",
    "    bar_width = 0.35\n",
    "    positions = np.arange(len(_df_apc))\n",
    "\n",
    "    # plot the bars for discounted apc on the first y-axis\n",
    "    bar1 = ax1.bar(positions - bar_width / 2, _df_apc[\"apc_saved\"], bar_width, label=\"Discounted APC (USD)\")\n",
    "    # plot the bars for number of publications on the second y-axis\n",
    "    bar2 = ax2.bar(positions + bar_width / 2, _df_apc[\"num_publications\"], bar_width, color=\"orange\", label=\"Number of Publications\")\n",
    "\n",
    "    # set the x-axis tick positions and labels\n",
    "    ax1.set_xticks(positions)\n",
    "    ax1.set_xticklabels(_df_apc.index, rotation=\"vertical\")\n",
    "\n",
    "    # set the x-axis label\n",
    "    ax1.set_xlabel(\"Publisher\")\n",
    "\n",
    "    # set the y-axis labels for both axes\n",
    "    ax1.set_ylabel(\"APC Saved (USD)\")\n",
    "    ax2.set_ylabel(\"Number of Publications\")\n",
    "\n",
    "    # add a combined legend for both bar plots\n",
    "    ax1.legend([bar1, bar2], [bar1.get_label(), bar2.get_label()], loc=\"upper right\")\n",
    "\n",
    "    # add value labels on top of the bars for number of publications\n",
    "    for bar in bar2:\n",
    "        yval = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width() / 2, yval, int(yval), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # set the title of the plot\n",
    "    plt.title(f\"Top {FIRST_NUM_ROWS} publishers by APC saved in {publication_year}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the total discounted APC cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_apc_discount = df_apc[\"discounted_apc_usd\"].sum()\n",
    "print(f\"Estimated APC cost (including discounts) for {publication_year}: USD {round(total_apc_discount, 2)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the total APC savings of our institution's read-publish agreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Estimated APC saving in {publication_year}: USD {round(total_apc - total_apc_discount, 2)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openalex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
